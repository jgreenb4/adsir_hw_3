---
title: "adsir_hw_3"
author: "Jake Greenberg"
date: "4/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(DALEX)             # for model interpretation  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
library(dbplyr)            # for SQL query "cheating" - part of tidyverse but needs to be loaded separately
library(mdsr)              # for accessing some databases - goes with Modern Data Science with R textbook
library(RMySQL)            # for accessing MySQL databases
library(RSQLite)           # for accessing SQLite databases
library(lime)

#mapping
library(maps)              # for built-in maps
library(sf)                # for making maps using geom_sf
library(ggthemes)          # Lisa added - I like theme_map() for maps :)

#tidytext
library(tidytext)          # for text analysis, the tidy way!
library(textdata)          
library(reshape2)
library(wordcloud)         # for wordcloud
library(stopwords)

theme_set(theme_minimal()) # Lisa's favorite theme
```

# Local Interpretable Machine Learning

You are going to use the King County house data and the same random forest model to predict log_price that I used in the tutorial.

Tasks:

Choose 3 new observations and do the following for each observation:
-Construct a break-down plot using the default ordering. Interpret the resulting graph. Which variables contribute most to each observation’s prediction?
    
    Construct a SHAP graph and interpret it. Does it tell a similar story to the break-down plot?
    Construct a LIME graph (follow my code carefully). How close is each original prediction to the prediction from the local model? Interpret the result. You can also try using fewer or more variables in the local model than I used in the example.

    Describe how you would use the interpretable machine learning tools we’ve learned (both local and global) in future machine learning projects? How does each of them help you?

```{r}
data("house_prices")

# Create log_price and drop price variable
house_prices <- house_prices %>% 
  mutate(log_price = log(price, base = 10)) %>% 
  # make all integers numeric ... fixes prediction problem
  mutate(across(where(is.integer), as.numeric)) %>% 
  select(-price)
```

```{r}
set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
house_split <- initial_split(house_prices, 
                             prop = .75)
house_training <- training(house_split)
house_testing <- testing(house_split)

# lasso recipe and transformation steps
house_recipe <- recipe(log_price ~ ., 
                       data = house_training) %>% 
  step_rm(sqft_living15, sqft_lot15) %>%
  step_log(starts_with("sqft"),
           -sqft_basement, 
           base = 10) %>% 
  step_mutate(grade = as.character(grade),
              grade = fct_relevel(
                        case_when(
                          grade %in% "1":"6"   ~ "below_average",
                          grade %in% "10":"13" ~ "high",
                          TRUE ~ grade
                        ),
                        "below_average","7","8","9","high"),
              basement = as.numeric(sqft_basement == 0),
              renovated = as.numeric(yr_renovated == 0),
              view = as.numeric(view == 0),
              waterfront = as.numeric(waterfront),
              age_at_sale = year(date) - yr_built)%>% 
  step_rm(sqft_basement, 
          yr_renovated, 
          yr_built) %>% 
  step_date(date, 
            features = "month") %>% 
  update_role(all_of(c("id",
                       "date",
                       "zipcode", 
                       "lat", 
                       "long")),
              new_role = "evaluative") %>% 
  step_dummy(all_nominal(), 
             -all_outcomes(), 
             -has_role(match = "evaluative")) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())

#define lasso model
house_lasso_mod <- 
  linear_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("regression")

# create workflow
house_lasso_wf <- 
  workflow() %>% 
  add_recipe(house_recipe) %>% 
  add_model(house_lasso_mod)

# create cv samples
set.seed(1211) # for reproducibility
house_cv <- vfold_cv(house_training, v = 5)


# penalty grid - changed to 10 levels
penalty_grid <- grid_regular(penalty(),
                             levels = 10)

# tune the model 
house_lasso_tune <- 
  house_lasso_wf %>% 
  tune_grid(
    resamples = house_cv,
    grid = penalty_grid
    )

# choose the best penalty
best_param <- house_lasso_tune %>% 
  select_best(metric = "rmse")

# finalize workflow
house_lasso_final_wf <- house_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
house_lasso_final_mod <- house_lasso_final_wf %>% 
  fit(data = house_training)

```

```{r}
# set up recipe and transformation steps and roles
ranger_recipe <- 
  recipe(formula = log_price ~ ., 
         data = house_training) %>% 
  step_date(date, 
            features = "month") %>% 
  # Make these evaluative variables, not included in modeling
  update_role(all_of(c("id",
                       "date")),
              new_role = "evaluative")

#define model
ranger_spec <- 
  rand_forest(mtry = 6, 
              min_n = 10, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

#create workflow
ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

#fit the model
set.seed(712) # for reproducibility - random sampling in random forest choosing number of variables
ranger_fit <- ranger_workflow %>% 
  fit(house_training)

```

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = house_lasso_final_mod,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "lasso"
  )
```

```{r}
# Create an explainer for the random forest model:
rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "rf"
  )
```
# PP Plot

## Random Observation #1

```{r message = FALSE, warning = FALSE}

# Choose an observation
new_obs_1 <- house_testing %>% slice(3425)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_1$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_1 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_1)

# Table form of break-down plot data
pp_lasso_1
```

## Random Observation #2

```{r message = FALSE, warning = FALSE}
# Choose an observation
new_obs_2 <- house_testing %>% slice(3921)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_2$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_2 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_2)

# Table form of break-down plot data
pp_lasso_2
```

## Random Observation #3

```{r message = FALSE, warning = FALSE}
# Choose an observation
new_obs_3 <- house_testing %>% slice(3425)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_3$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_3 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_3)

# Table form of break-down plot data
pp_lasso_3
```

# SHAP

## Observation 1

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_1,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

## Observation 2

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_2,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

## Observation 3

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_3,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

# LIME

## Observation 1

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_1 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_1 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_1 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_1) +
  labs(x = "Variable")
```

## Observation 2

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_2 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_2 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_2 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_2) +
  labs(x = "Variable")
```

## Observation 3

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_3 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_3 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_3 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_3) +
  labs(x = "Variable")
```

2. Describe how you would use the interpretable machine learning tools we’ve learned (both local and global) in future machine learning projects? How does each of them help you?

# SQL

You will use the airlines data from the SQL database that I used in the example in the tutorial. Be sure to include the chunk to connect to the database here. And, when you are finished, disconnect. You may need to reconnect throughout as it times out after a while.


1. Create a SQL chunk and an equivalent R code chunk that does the following: for each airport (with its name, not code), year, and month find the total number of departing flights, the distinct destinations to which they flew, the average length of the flight, the average distance of the flight, and the proportion of flights that arrived more than 20 minutes late. In the R code chunk, write this out to a dataset. (HINT: 1. start small! 2. you may want to do the R part first and use it to “cheat” into the SQL code).

```{r}
con_air <- dbConnect(RMySQL::MySQL(), 
                     dbname = "airlines", 
                     host = "mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com", 
                     user = "mdsr_public", 
                     password = "ImhsmflMDSwR")


con_air <- dbConnect_scidb("airlines")
```

```{r}
dbListTables(con_air)
```

```{sql connection= con_air}
SHOW TABLES;
```

```{r}
dbListFields(con_air, "flights")
```

```{sql connection= con_air}
DESCRIBE flights;
```

- With the dataset you wrote out, create a graph that helps illustrate the “worst” airports in terms of late arrivals. You have some freedom in how you define worst and you may want to consider some of the other variables you computed. Do some theming to make your graph look glamorous (those of you who weren’t in my intro data science class this year may want to watch Will Chase’s Glamour of Graphics talk for inspiration).

- Although your graph was truly inspirational, you’ve been requested to “boil it down to a few numbers.” Some people just don’t appreciate all that effort you put in. And, you need to use the already summarized data that you already pulled in from SQL. Create a table with 6 or fewer rows and 3 or fewer columns that summarizes which airport is the “worst” in terms of late arrivals. Be careful with your calculations. You may consider using the kable, kableExtra, or gt packages to make your table look truly spectacular.

2. Come up with your own interesting question that data in the airlines database can help you answer. Write a SQL query and equivalent R code chunk to extract the data you need and create an elegant graph to help answer the question. Be sure to write down the question so it is clear.
