---
title: "adsir_hw_3"
author: "Jake Greenberg"
date: "4/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(DALEX)             # for model interpretation  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
library(dbplyr)            # for SQL query "cheating" - part of tidyverse but needs to be loaded separately
library(mdsr)              # for accessing some databases - goes with Modern Data Science with R textbook
library(RMySQL)            # for accessing MySQL databases
library(RSQLite)           # for accessing SQLite databases
library(lime)
library(plotly)
library(formattable)
library(kableExtra)

#mapping
library(maps)              # for built-in maps
library(sf)                # for making maps using geom_sf
library(ggthemes)          # Lisa added - I like theme_map() for maps :)

#tidytext
library(tidytext)          # for text analysis, the tidy way!
library(textdata)          
library(reshape2)
library(wordcloud)         # for wordcloud
library(stopwords)

theme_set(theme_minimal()) # Lisa's favorite theme
```

# Local Interpretable Machine Learning

You are going to use the King County house data and the same random forest model to predict log_price that I used in the tutorial.

```{r}
data("house_prices")

# Create log_price and drop price variable
house_prices <- house_prices %>% 
  mutate(log_price = log(price, base = 10)) %>% 
  # make all integers numeric ... fixes prediction problem
  mutate(across(where(is.integer), as.numeric)) %>% 
  select(-price)
```

```{r}
set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
house_split <- initial_split(house_prices, 
                             prop = .75)
house_training <- training(house_split)
house_testing <- testing(house_split)

# lasso recipe and transformation steps
house_recipe <- recipe(log_price ~ ., 
                       data = house_training) %>% 
  step_rm(sqft_living15, sqft_lot15) %>%
  step_log(starts_with("sqft"),
           -sqft_basement, 
           base = 10) %>% 
  step_mutate(grade = as.character(grade),
              grade = fct_relevel(
                        case_when(
                          grade %in% "1":"6"   ~ "below_average",
                          grade %in% "10":"13" ~ "high",
                          TRUE ~ grade
                        ),
                        "below_average","7","8","9","high"),
              basement = as.numeric(sqft_basement == 0),
              renovated = as.numeric(yr_renovated == 0),
              view = as.numeric(view == 0),
              waterfront = as.numeric(waterfront),
              age_at_sale = year(date) - yr_built)%>% 
  step_rm(sqft_basement, 
          yr_renovated, 
          yr_built) %>% 
  step_date(date, 
            features = "month") %>% 
  update_role(all_of(c("id",
                       "date",
                       "zipcode", 
                       "lat", 
                       "long")),
              new_role = "evaluative") %>% 
  step_dummy(all_nominal(), 
             -all_outcomes(), 
             -has_role(match = "evaluative")) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())

#define lasso model
house_lasso_mod <- 
  linear_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("regression")

# create workflow
house_lasso_wf <- 
  workflow() %>% 
  add_recipe(house_recipe) %>% 
  add_model(house_lasso_mod)

# create cv samples
set.seed(1211) # for reproducibility
house_cv <- vfold_cv(house_training, v = 5)


# penalty grid - changed to 10 levels
penalty_grid <- grid_regular(penalty(),
                             levels = 10)

# tune the model 
house_lasso_tune <- 
  house_lasso_wf %>% 
  tune_grid(
    resamples = house_cv,
    grid = penalty_grid
    )

# choose the best penalty
best_param <- house_lasso_tune %>% 
  select_best(metric = "rmse")

# finalize workflow
house_lasso_final_wf <- house_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
house_lasso_final_mod <- house_lasso_final_wf %>% 
  fit(data = house_training)

```

```{r}
# set up recipe and transformation steps and roles
ranger_recipe <- 
  recipe(formula = log_price ~ ., 
         data = house_training) %>% 
  step_date(date, 
            features = "month") %>% 
  # Make these evaluative variables, not included in modeling
  update_role(all_of(c("id",
                       "date")),
              new_role = "evaluative")

#define model
ranger_spec <- 
  rand_forest(mtry = 6, 
              min_n = 10, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

#create workflow
ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

#fit the model
set.seed(712) # for reproducibility - random sampling in random forest choosing number of variables
ranger_fit <- ranger_workflow %>% 
  fit(house_training)

```

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = house_lasso_final_mod,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "lasso"
  )
```

```{r}
# Create an explainer for the random forest model:
rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "rf"
  )
```
# PP Plot

## Random Observation #1

```{r message = FALSE, warning = FALSE}

# Choose an observation
new_obs_1 <- house_testing %>% slice(3425)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_1$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_1 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_1)

# Table form of break-down plot data
pp_lasso_1
```

**The variables that contribute the most to this observation's ultimate prediction are its grade, the year in which it was built, and the square feet of living it possesses.**

## Random Observation #2

```{r message = FALSE, warning = FALSE}
# Choose an observation
new_obs_2 <- house_testing %>% slice(3921)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_2$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_2 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_2)

# Table form of break-down plot data
pp_lasso_2
```

**The variables that contribute the most to this observation's ultimate prediction are its grade, the year in which it was built, and the square feet of living it possesses.**

## Random Observation #3

```{r message = FALSE, warning = FALSE}
# Choose an observation
new_obs_3 <- house_testing %>% slice(3425)

# Price of new_obs's house - just to know because I can't think in logs
10^(new_obs_3$log_price)

# Pulls together the data needed for the break-down plot
pp_lasso_3 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs_1,
                          type = "break_down") #default

# Break-down plot
plot(pp_lasso_3)

# Table form of break-down plot data
pp_lasso_3
```

**The variables that contribute the most to this observation's ultimate prediction are also its grade, the year in which it was built, and the square feet of living it possesses. Considering that this has been consistent for each of my randomly selected observations, it would seem as though these three variables are fairly crucial in arriving at the ultiamte prediction.**

# SHAP

## Observation 1

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_1,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

## Observation 2

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_2,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

## Observation 3

```{r}
rf_shap <-predict_parts(explainer = rf_explain,
                        new_observation = new_obs_3,
                        type = "shap",
                        B = 10 #number of reorderings - start small
)

plot(rf_shap)
```

### Construct a SHAP graph and interpret it. Does it tell a similar story to the break-down plot?

**Each of the SHAPs above tells a somewhat similar story to the PP plots above, in that they exhibit the importance of square feet of living and grade, but they also show that the geographic location of the home (longitude and lattitude), which could serve as somehwatof a proxy for neighborhood location, is also an important determinant. Meanwhile, these SHAP plots seem to undermine the importance of the year when the house was built, a variable that seemed quite important in the PP plots.**

# LIME

## Observation 1

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_1 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_1 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_1 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_1) +
  labs(x = "Variable")
```

## Observation 2

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_2 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_2 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_2 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_2) +
  labs(x = "Variable")
```

## Observation 3

```{r}
set.seed(2)

# NEED these two lines of code always!
# They make sure our explainer is defined correctly to use in the next step
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_rf_3 <- predict_surrogate(explainer = rf_explain,
                             new_observation = new_obs_3 %>%
                               select(-log_price), 
                             n_features = 5,
                             n_permutations = 1000,
                             type = "lime")

lime_rf_3 %>% 
  select(model_r2, model_prediction, prediction) %>% 
  distinct()



plot(lime_rf_3) +
  labs(x = "Variable")
```

### Construct a LIME graph (follow my code carefully). How close is each original prediction to the prediction from the local model? Interpret the result. You can also try using fewer or more variables in the local model than I used in the example.

**Overall, each of the original predictions is fairly close to the predictions from the local model. Based on the LIME graphs, it seems that there is a breaking point somewhere on the spectrum of square feet of living where the variable goes from carrying a negative weight to the final prediction to possessing a positive one (somewhere between 1930 and 2560).**

### 2. Describe how you would use the interpretable machine learning tools we’ve learned (both local and global) in future machine learning projects? How does each of them help you?

**I would use the interpretable machine learning tools we've learned (both local and global) in future machine learning projects to develop a more clear understanding of the modeling process and how my model is ultimately making predictions. I think these could be particularly effective for diagnosing the reasoning behind possible bias in a model's predictive accuracy or why it could be missing specific observations by so much, to further improve its thoroughness.**

# SQL

You will use the airlines data from the SQL database that I used in the example in the tutorial. Be sure to include the chunk to connect to the database here. And, when you are finished, disconnect. You may need to reconnect throughout as it times out after a while.


1. Create a SQL chunk and an equivalent R code chunk that does the following: for each airport (with its name, not code), year, and month find the total number of departing flights, the distinct destinations to which they flew, the average length of the flight, the average distance of the flight, and the proportion of flights that arrived more than 20 minutes late. In the R code chunk, write this out to a dataset. (HINT: 1. start small! 2. you may want to do the R part first and use it to “cheat” into the SQL code).


```{r}
con_air <- dbConnect(RMySQL::MySQL(), 
                     dbname = "airlines", 
                     host = "mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com", 
                     user = "mdsr_public", 
                     password = "ImhsmflMDSwR")


con_air <- dbConnect_scidb("airlines")
```

```{r}
dbListTables(con_air)
```

```{sql connection= con_air}
SHOW TABLES;
```

```{r}
dbListFields(con_air, "flights")
```

```{sql connection= con_air}
DESCRIBE flights;
```

Create a SQL chunk and an equivalent R code chunk that does the following: for each airport (with its name, not code), year, and month find the total number of departing flights, the distinct destinations to which they flew, the average length of the flight, the average distance of the flight, and the proportion of flights that arrived more than 20 minutes late. In the R code chunk, write this out to a dataset. (HINT: 1. start small! 2. you may want to do the R part first and use it to “cheat” into the SQL code).
 
 
```{r}
airports_query <- 
tbl(con_air,sql("select `origin`, `name`, `month`, `total_departures`, `distinct_dest`, `avg_dist`, `prop_late_over20`
from (select `LHS`.`origin`, `LHS`.`month`, `LHS`.`total_departures`, `LHS`.`avg_dist`, `LHS`.`prop_late_over20`, `LHS`.`distinct_dest`, `RHS`.`name`, `RHS`.`lat`, `RHS`.`lon`, `RHS`.`alt`, `RHS`.`tz`, `RHS`.`dst`, `RHS`.`city`, `RHS`.`country`
FROM (select `origin`, `month`, count(*) as `total_departures`, avg(`distance`) as `avg_dist`, avg(`arr_delay` > 20) as `prop_late_over20`, count(distinct dest) as `distinct_dest`
FROM `flights`
WHERE (`year` = 2017)
GROUP BY `origin`, `month`) `LHS`
INNER JOIN `airports` AS `RHS`
ON (`LHS`.`origin` = `RHS`.`faa`)
)`dbplyr_027`"))

airports_query
```
 
```{r}
show_query(airports_query)
```


```{r}
airports_query_collect <- collect(airports_query)
```

 

- With the dataset you wrote out, create a graph that helps illustrate the “worst” airports in terms of late arrivals. You have some freedom in how you define worst and you may want to consider some of the other variables you computed. Do some theming to make your graph look glamorous (those of you who weren’t in my intro data science class this year may want to watch Will Chase’s Glamour of Graphics talk for inspiration).

```{r}
airports_query_collect_final <- airports_query_collect %>% 
  mutate(month = as.factor(month)) %>% 
  group_by(name) %>% 
  filter(total_departures >= 20) %>% 
  summarize(mean_proportion = mean(prop_late_over20), mean_dist = mean(avg_dist))
```

```{r}
freq_dist_plot <- ggplot(airports_query_collect_final, aes(x = mean_proportion,
             y = mean_dist, color = name)) +
  geom_point() +
  scale_x_continuous(expand = c(0,0), 
                     labels = scales::percent) + 
  xlab("Proportion of Late Flights (20 + Minutes)") +
  ylab("Average Distance Travelled (Miles)") +
  theme(legend.position = "none") +
  ggtitle("Relationship Between Distance Traveled and Proportion of Flights 20+ Minutes Late ")
ggplotly(freq_dist_plot)
```


- Although your graph was truly inspirational, you’ve been requested to “boil it down to a few numbers.” Some people just don’t appreciate all that effort you put in. And, you need to use the already summarized data that you already pulled in from SQL. Create a table with 6 or fewer rows and 3 or fewer columns that summarizes which airport is the “worst” in terms of late arrivals. Be careful with your calculations. You may consider using the kable, kableExtra, or gt packages to make your table look truly spectacular.

```{r}
query_wrangled <- airports_query_collect %>%
  mutate(month_category = month, month_category = replace(month_category, month_category %in% c(1,2), "January & February"), month_category = replace(month_category, month_category %in% c(3,4), "March & April"), month_category = replace(month_category, month_category %in% c(5,6), "May & June"), month_category = replace(month_category, month_category %in% c(7,8), "July & August"), month_category = replace(month_category, month_category %in% c(9,10), "September & October"), month_category = replace(month_category, month_category %in% c(11,12), "November & December"), month_category = factor(month_category, levels = c("January & February", "March & April","May & June", "July & August", "September & October", "November & December"))) %>% 
  group_by(month_category) %>% 
  rename(`Month Grouping` = month_category) %>% 
  summarize(`Proportion of Flights Delayed Over 20 Minutes` = mean(prop_late_over20))

query_table <- formattable(query_wrangled[1:6, 1:2], color = "Black", list( `Player` = formatter("span", style = ~ style(color
= "#2F241D",font.weight = "bold"))))
kable(query_table) %>%
kableExtra::row_spec(0, color = "#FFC425", background = "#2F241D") %>%
kableExtra::kable_styling(full_width = FALSE) %>%
kableExtra::add_header_above(c("Final Model Training Accuracies" = 2), color = "#FFC425", background = "#2F241D")
```


2. Come up with your own interesting question that data in the airlines database can help you answer. Write a SQL query and equivalent R code chunk to extract the data you need and create an elegant graph to help answer the question. Be sure to write down the question so it is clear.

## **Are there certain years in which there are more flights canceled than others?**

```{r}
airports_new_query <- 
tbl(con_air,sql("select `origin`, `name`, `month`, `total_departures`, `avg_dist`, `total_cancellations`, `year`
from (select `LHS`.`origin`, `LHS`.`month`, `LHS`.`total_departures`, `LHS`.`avg_dist`, `LHS`.`year`, `LHS`.`total_cancellations`, `RHS`.`name`, `RHS`.`lat`, `RHS`.`lon`, `RHS`.`alt`, `RHS`.`tz`, `RHS`.`dst`, `RHS`.`city`, `RHS`.`country`
FROM (select `origin`, `month`, `year`, count(*) as `total_departures`, avg(`distance`) as `avg_dist`,  sum(`cancelled`) as total_cancellations
FROM `flights`
GROUP BY `origin`, `month`) `LHS`
INNER JOIN `airports` AS `RHS`
ON (`LHS`.`origin` = `RHS`.`faa`)
)`dbplyr_027`"))
```

```{r}
airports_new_query_collect <- collect(airports_new_query)
```

```{r}
airports_new_query_collect %>% 
  group_by(year) %>% 
  summarize(total_cancellations = sum(total_cancellations), total_departures = sum(total_departures), pct = 100* total_cancellations/total_departures) %>% 
  ggplot(aes(x = year, y = pct)) + geom_point() + geom_line() + labs(x = "Year", y = "Percentage of Flights Canceled") + ggtitle("Percentage of Canceled Flights by Year")
```

```{r}
dbDisconnect(con_air)
```


# Function Friday

## geom_sf

**This was my group's project, so I was not sure if I needed to do these exercises.**

-Change the color scheme of the map from the default blue (one option could be viridis).
-Add a dot (or any symbol you want) to the centroid of each state.
-Add a layer onto the map with the counties.
-`Change the coordinates of the map to zoom in on your favorite state.

```{r}
states <- st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
head(states)
```

```{r}
ggplot(data = states) +
    geom_sf(fill = NA) +
    coord_sf(xlim = c(-127, -63), ylim = c(24, 51), expand = FALSE)

states <- states %>%
  mutate(area = as.numeric(st_area(states)))

ggplot(data = states) +
    geom_sf(aes(fill = area)) +
    coord_sf(xlim = c(-127, -63), ylim = c(24, 51), expand = FALSE)
```

Question 1:

```{r}
ggplot(data = states) +
    geom_sf(aes(fill = area)) +
    scale_fill_viridis_c(trans = "sqrt", alpha = .4) +
    coord_sf(xlim = c(-127, -63), ylim = c(24, 51), expand = FALSE)
```

Question 2:

```{r}
ggplot(data = states) +
    geom_sf(aes(fill = area)) +
    stat_sf_coordinates() +
    coord_sf(xlim = c(-127, -63), ylim = c(24, 51), expand = FALSE)
```

Question 3:

```{r}
counties <- st_as_sf(maps::map("county", plot = FALSE, fill = TRUE))
head(counties)

ggplot(data = states) +
    geom_sf(fill = NA) +
    geom_sf(data = counties)

counties <- counties %>%
  mutate(area = as.numeric(st_area(counties)))

ggplot(data = states) +
    geom_sf(data = counties, aes(fill = area)) +
    coord_sf(xlim = c(-127, -63), ylim = c(24, 51), expand = FALSE)
```

Question 4:

Example -> Washington State

```{r}
ggplot(data = states) +
    geom_sf(aes(fill = area)) +
    coord_sf(xlim = c(-125, -115), ylim = c(45, 50), expand = FALSE)
```

## tidytext

Now you will try using tidytext on a new dataset about Russian Troll tweets.
Read about the data

These are tweets from Twitter handles that are connected to the Internet Research Agency (IRA), a Russian “troll factory.” The majority of these tweets were posted from 2015-2017, but the datasets encompass tweets from February 2012 to May 2018.

Three of the main categories of troll tweet that we will be focusing on are Left Trolls, Right Trolls, and News Feed. Left Trolls usually pretend to be BLM activists, aiming to divide the democratic party (in this context, being pro-Bernie so that votes are taken away from Hillary). Right trolls imitate Trump supporters, and News Feed handles are “local news aggregators,” typically linking to legitimate news.

For our upcoming analyses, some important variables are:

    author (handle sending the tweet)
    content (text of the tweet)
    language (language of the tweet)
    publish_date (date and time the tweet was sent)

Variable documentation can be found on Github and a more detailed description of the dataset can be found in this fivethirtyeight article.

Because there are 12 datasets containing 2,973,371 tweets sent by 2,848 Twitter handles in total, we will be using three of these datasets (one from a Right troll, one from a Left troll, and one from a News Feed account).



1. Read in Troll Tweets Dataset - this takes a while. You can cache it so you don’t need to read it in again each time you knit. Be sure to remove the eval=FALSE!!!!

```{r}
troll_tweets <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv")
```


2. Basic Data Cleaning and Exploration

Remove rows where the tweet was in a language other than English

```{r}
troll_tweets_cleaned <- troll_tweets %>% 
  filter(language == "English")
```

Report the dimensions of the dataset
```{r}
dim(troll_tweets_cleaned)
```

Create two or three basic exploratory plots of the data (ex. plot of the different locations from which tweets were posted, plot of the account category of a tweet)

```{r}
troll_tweets_cleaned %>% 
  group_by(account_category) %>% 
  mutate(total = n()) %>% 
  ggplot(aes(x = account_category, y = total)) +
  coord_flip() +
  geom_bar(stat = "identity", fill = "red") +
  labs(y = "Total Number of Tweets", x = "Category of Account") +
  ggtitle("Relationship Between Account Category and Tweet Frequency within Dataset")
```

```{r}
troll_tweets_cleaned %>% 
  mutate(publish_month = substr(publish_date, 1, 2), publish_month = gsub("/","", publish_month), publish_month = factor(publish_month, levels = c("12", "11", "10", "9", "8", "7", "6", "5", "4", "3", "2", "1"))) %>% 
  group_by(publish_month) %>% 
  mutate(total = n()) %>% 
  ggplot(aes(x = publish_month, y = total)) +
  geom_bar(stat ="identity", color = "orange") +
  coord_flip() + 
  labs(x = "Month of Tweet Publishing", y = "Total Count of Tweets") +
  ggtitle("Relationship Between Month of Year and Tweet Frequency in Dataset")
```

3. Unnest Tokens

```{r}
troll_tweets_untoken <- troll_tweets_cleaned %>% 
  unnest_tokens(word, content)

troll_tweets_untoken
```

4. Remove stopwords. Be sure to remove the eval=FALSE!!!!

```{r}
#get rid of stopwords (the, and, etc.)
troll_tweets_cleaned <- troll_tweets_untoken %>%
  anti_join(stop_words)
```

4 (Cont'd). Take a look at the troll_tweets_cleaned dataset. Are there any other words/letters/numbers that we want to eliminate that weren’t taken care of by stop_words? Be sure to remove the eval=FALSE!!!!

```{r}
#get rid of http, https, t.co, rt, amp, single number digits, and singular letters
troll_tweets_cleaned <- troll_tweets_cleaned %>%
  filter(!word %in% c("https", "t.co", "lldda0raa7")) # you can use %in% for a list of words
```


5. Look at a subset of the tweets to see how often the top words appear.

```{r}
troll_tweets_small <- troll_tweets_cleaned %>%
  count(word) %>%
  slice_max(order_by = n, n = 50) # 50 most occurring words

# visualize the number of times the 50 top words appear
ggplot(troll_tweets_small, 
       aes(y = fct_reorder(word,n), x = n)) +
  geom_col()
```


### Sentiment Analysis

a. Get the sentiments using the “bing” parameter (which classifies words into “positive” or “negative”).

```{r}
# look at sentiment
sentiments <- get_sentiments("bing")

# assign a sentiment to each word that has one associated
troll_tweets_sentiment <- troll_tweets_cleaned %>%
  inner_join(sentiments, by = "word")
```

b. Report how many positive and negative words there are in the dataset. Are there more positive or negative words, and why do you think this might be?

**There are far more negative words in the dataset than positive words. This could be due to the fact that the motivation for creating these accounts that will post "troll tweets" could be to sabotage a political opponent in the public light, which would explain the large proportion of words with negative sentiments.**

```{r}
# count the sentiments
troll_tweets_sentiment %>% 
  group_by(sentiment) %>% 
  summarize(count = n())
```

7. Using the troll_tweets_small dataset, make a wordcloud:

  a. That is sized by the number of times that a word appears in the tweets
  b. That is colored by sentiment (positive or negative)
  
```{r}
# make a wordcloud where the size of the word is based on the number of times the word appears across the tweets
troll_tweets_small %>%
  with(wordcloud(word, n, max.words = 800))

# make a wordcloud colored by sentiment

troll_tweets_sentiment %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red","blue"),
                   max.words = 100)
```

Are there any words whose categorization as “positive” or “negative” surprised you?

**I was not really surprised by any of the sentiments. Perhaps, a word like "conservative" is surprising in that it lies on one end of the spectrum (negative), instead of being more neutral.**

# Projects

At the end of every season, popular EA Sports video game FIFA releases its TOTS Awards, or Teams of the Season, for each league. These teams are generally supposed to be reflective of who the best players at each of their positions were in the league, but due to the profitability motivations of EA Sports (people pay real money for the chance to acquire these "special players" in the video game, so this could be motivation for the company to favor more popular players/players from teams with greater popularity in their selections). My group is going to be working with soccer data from a variety of sources (Football Reference, FIFA data, attendance/jersey sales) to both derive the primary components of being selected to a FIFA team of the season for each of the top 5 leagues, and have predictive power at estimating who should be on the team of the season for future seasons. 

# "Undoing Bias"

I think that this tweet from Sara Hooker relates quite strongly to the sentiments presented in the Coded Bias film. This "debate" that she found herself in is an absolutely imperative one because it relates to accountability and admittance of control over whether an algorithim will ultimately exhibit bias. As a data scientist, one has a certain ethical and statistical validity-based obligation to ensure that their models minimize potential biases throughout each step in the modeling process. One example of such biases that Hooker references in her tweet, which has been frequently discussed throughout this module, has been the necessity for accurate demographic representation within a training dataset- this is an example of an issue that has both ethical and statistical-accuracy implications that feed into each other, because if a certain demographic for a characteristic within the population is under-represented in the dataset and someone tries to apply an algorithm to people fitting that description, it could have real-world consequences due to these inaccuracies.
